package com.lxmt.multiinsert;
import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.KeyValue;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2;
import org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles;
import org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat;
import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;
import org.apache.hadoop.mapred.TextOutputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;



public class HbaseMultiTableBulkInsert {
	
	
	
	
	
	
public static void main(String[] args) throws Exception {
	Configuration conf = new Configuration();
	conf.set("mapreduce.task.io.sort.mb", "512M");
	conf.set("mapreduce.map.java.opts", "-Xmx512M");
	
    args = new GenericOptionsParser(conf, args).getRemainingArgs();
    // Load hbase-site.xml 
    HBaseConfiguration.addHbaseResources(conf);
    Job job = Job.getInstance(conf,"Bulk Insert");
    job.setJarByClass(HbaseMultiTableBulkInsert.class);
    job.setMapperClass(BulkLoadMapper.class);
    job.setReducerClass(BulkLoadReducer.class);
    job.setMapOutputKeyClass(ImmutableBytesWritable.class);
    job.setMapOutputValueClass(Put.class);
    job.setOutputFormatClass(MultiTableOutputFormat.class);
    TableMapReduceUtil.addDependencyJars(job);
    TableMapReduceUtil.addDependencyJars(job.getConfiguration());
    FileInputFormat.addInputPath(job, new Path(args[0]));
    job.waitForCompletion(true);
    HTable table1 = new HTable(conf,"Table1");
    HTable table2 = new HTable(conf,"Table1");
    LoadIncrementalHFiles load=new LoadIncrementalHFiles(conf);
    load.doBulkLoad(new Path("output1"), table1);
    load.doBulkLoad(new Path("output2"), table2);
}
}
