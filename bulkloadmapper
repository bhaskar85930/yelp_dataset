package com.lxmt.multiinsert;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.KeyValue;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;

import java.io.ByteArrayOutputStream;
import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.util.Enumeration;
import java.util.Random;
import java.util.zip.ZipEntry;
import java.util.zip.ZipFile;

public class BulkLoadMapper extends Mapper<LongWritable, Text, ImmutableBytesWritable, KeyValue> {

	byte[] buf = new byte[1024];
	MultipleOutputs mos;

	@Override
	protected void setup(Context context) throws IOException, InterruptedException {
		Configuration conf = context.getConfiguration();
		mos = new MultipleOutputs(context);
	}

	@Override
	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
		String[] valuePair = value.toString().split(":");
		KeyValue kv = new KeyValue();
		ImmutableBytesWritable hKey = new ImmutableBytesWritable();
		if (valuePair[0].startsWith("A")) {

			hKey.set(Bytes.toBytes("Table1"));

		} else {
			hKey.set(Bytes.toBytes("Table2"));
		}
		kv = new KeyValue(Bytes.toBytes("rowKey1"), Bytes.toBytes("colFam1"), Bytes.toBytes("col1"),
				Bytes.toBytes(valuePair[1]));
		context.write(hKey, kv);
	}
	
	

}
